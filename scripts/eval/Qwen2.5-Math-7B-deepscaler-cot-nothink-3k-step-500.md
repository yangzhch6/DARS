/hpc2hdd/home/zyang398/yangzhch6/projs/reasoning_baselines/rllm/checkpoints/reasoning_baselines/Qwen2.5-Math-7B-deepscaler-cot-nothink-3k/global_step_500/actor_huggingface

("Initial validation metrics: {'val/test_score/math': 0.78675, "
"'val_global_response_length_mean/math': 567.04075, "
"'val_correct_length_mean/math': 482.8322211630124, "
"'val_incorrect_length_mean/math': 877.7139507620165, "
"'val/test_score/olympiad_bench': 0.4281203566121842, "
"'val_global_response_length_mean/olympiad_bench': 766.6233283803863, "
"'val_correct_length_mean/olympiad_bench': 666.7947939262473, "
"'val_incorrect_length_mean/olympiad_bench': 841.356934069503, "
"'val/test_score/minerva': 0.34099264705882354, "
"'val_global_response_length_mean/minerva': 575.5689338235294, "
"'val_correct_length_mean/minerva': 464.45013477088946, "
"'val_incorrect_length_mean/minerva': 633.0655509065551, "
"'val/test_score/aime': 0.33684895833333334, "
"'val_global_response_length_mean/aime': 1058.1227864583334, "
"'val_correct_length_mean/aime': 815.014302280634, "
"'val_incorrect_length_mean/aime': 1181.6102493618691, 'val/test_score/amc': "
"0.6027861445783133, 'val_global_response_length_mean/amc': "
"807.7123964608434, 'val_correct_length_mean/amc': 632.8260462211118, "
"'val_incorrect_length_mean/amc': 1073.1086492890995, "
"'val/test_score/aime25': 0.09466145833333334, "
"'val_global_response_length_mean/aime25': 1101.0239583333334, "
"'val_correct_length_mean/aime25': 1052.0288858321871, "
"'val_incorrect_length_mean/aime25': 1106.146843089314, 'avg_score': "
'0.4316932608193313}')